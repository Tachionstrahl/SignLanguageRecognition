{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction with .h5 and .tflite file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import keras\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import tools\n",
    "\n",
    "# Root CSV files directory\n",
    "dirname = \"./prediction_data/\"\n",
    "#dirname = \"./data/\"\n",
    "\n",
    "# Frame count\n",
    "frames = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100, 64)           38656     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 32)           12416     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 59,557\n",
      "Trainable params: 59,557\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('sign_lang_recognition.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the file to predict and tokenize labels for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'computer', 2: 'deutschland', 3: 'hallo', 4: 'welt'}\n"
     ]
    }
   ],
   "source": [
    "listfile = os.listdir(dirname)\n",
    "data = []\n",
    "for wordname in listfile:\n",
    "    if wordname == \".DS_Store\":\n",
    "        continue\n",
    "    for csv in os.listdir(dirname + wordname):\n",
    "        filepath = os.path.join(dirname, wordname, csv)\n",
    "        content = pd.read_csv(filepath, sep=';')\n",
    "        content = content.reindex(list(range(0, frames)), fill_value=0.0)\n",
    "        content.fillna(0.0, inplace = True) \n",
    "        data.append((wordname, content))\n",
    "features = [n[1] for n in data]\n",
    "features = [f.to_numpy() for f in features]\n",
    "labels = [n[0] for n in data]\n",
    "x_train=np.array(features, dtype=\"float32\")\n",
    "\n",
    "with open('tokens_json.txt', 'r') as outfile:\n",
    "    json_ex = outfile.read()\n",
    "    \n",
    "#tokenizer = tools.tokenize(dirname)\n",
    "tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(json_ex)\n",
    "token_labels = {y:x for x,y in tokenizer.word_index.items()}\n",
    "print(token_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listfile = os.listdir(dirname)\n",
    "data = []\n",
    "for wordname in listfile:\n",
    "    if wordname == \".DS_Store\":\n",
    "        continue\n",
    "    for csv in os.listdir(dirname + wordname):\n",
    "        filepath = os.path.join(dirname, wordname, csv)\n",
    "        content = pd.read_csv(filepath, sep=';')\n",
    "        content = content.reindex(list(range(0, frames)), fill_value=0.0)\n",
    "        content.fillna(0.0, inplace = True) \n",
    "        data.append((wordname, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [n[1] for n in data]\n",
    "features = [f.to_numpy() for f in features]\n",
    "x_train=np.array(features, dtype=\"float32\")\n",
    "print(len(x_train[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([np.argmax(pred) for pred in y_pred])\n",
    "print(predictions)\n",
    "print([token_labels[p] for p in predictions])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/lite/guide/inference#load_and_run_a_model_in_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"sign_lang_recognition.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(input_details)\n",
    "# Test model on random input data.\n",
    "\n",
    "#interpreter.set_tensor(0.0, x_train)\n",
    "\n",
    "input_shape = input_details[0]['shape']\n",
    "time1= time()\n",
    "input_data = [1]\n",
    "predictions = []\n",
    "for x_pred in x_train:\n",
    "    input_data[0] = x_pred\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions.append(np.argmax(output_data))\n",
    "predictions = [token_labels[p] for p in predictions]\n",
    "print(\"Predictions:\", predictions)\n",
    "time2= time()\n",
    "dauer = time2 - time1\n",
    "print(\"Anzahl Samples: \", len(predictions))\n",
    "print(\"Predictiondauer: \", dauer)\n",
    "print(\"Samples pro Sekunde:\", (len(predictions)/dauer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
