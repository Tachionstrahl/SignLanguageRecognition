# SignLanguageRecognition

This repository contains a variety of tools to build up a experimental ecosystem for recognizing signs of the german sign language (DGS).
Our claim is an experimental attempt at live subtitling of gestures.
For this we train a deep learning model (RNN) for predicting the actual signs made by a person filmed.
Therefore we use [MediaPipe](https://github.com/google/mediapipe), a framework for building ML pipelines, to extract face and hand positions, including multiple coordinates for each finger.

## Installation

This repository is only tested under `Linux` (CPU and GPU) and `macOS` (CPU only).

1. Clone the repository.
2. Follow the instructions to [install MediaPipe](https://google.github.io/mediapipe/getting_started/install).
3. To work with our `jupyter notebooks`, we recommend to install [Anaconda](https://www.anaconda.com/).
4. Install `TensorFlow 2.2.0` with `conda`, see <https://anaconda.org/anaconda/tensorflow-gpu>

## Workflow

### 1. Gathering video examples

For training we need many videos for each sign, we want to predict. Those examples are generated by users of our platform [Geb√§rdenfutter](https://gebaerdenfutter.de).

### 2. Extracting face and hand positions

For extracting multi hand and face detections for each frame of the videos and saving them, we built a pipeline with `MediaPipe`, e.g. have a look at the `DetectionsToCSVCalculator`, we implemented. It simply writes out the detections made by `MediaPipe` to CSV files.

### 3. Training deep learning model

The CSV files are used to train a deep learning model with `Keras`, a high level API for `TensorFlow`.
Therefore we use jupyter notebooks to simply write and comment scripts.
Check out the folder `lab`.

### 4. Live prediction (Subtitling) ***Work in progress***
<img alt="SignLang Predictino Graph" src="docs/sign_lang_graph.png" width="500px">
Visualization of MediaPipe Graph

The trained model is used for predicting live video stream. See the `SignLanguageRecognitionCalculator` for further details on how we try to use the model for live predictions. Currently it's not working well, like we expected before, but it provides us an infrastructure for experiments and testing. You've got ideas for improvements? Let us know!
